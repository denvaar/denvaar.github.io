<!DOCTYPE html>
<html>
  <head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script
      async
      src="https://www.googletagmanager.com/gtag/js?id=G-P0BCBS2V0C"
    ></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() {
        dataLayer.push(arguments);
      }
      gtag("js", new Date());

      gtag("config", "G-P0BCBS2V0C");
    </script>
    <title>Makeshift Security Camera</title>
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <link type="image/x-icon" rel="icon" href="favicon.ico" />
    <link
      href="https://fonts.googleapis.com/css?family=IBM+Plex+Mono:400,600,400i&display=swap"
      rel="stylesheet"
    />
    <link rel="stylesheet" href="../css/article.css" />
    <link rel="stylesheet" href="../css/highlightjs/default.css" />
  </head>
  <body>
    <div class="article">
      <div class="top-nav">
        
        <div>
          <a alt="Exploring Text Compression" href="exploring_text_compression.html"
            >Previous</a
          >
        </div>
        
        <div>
          <a href="../articles/index.html">All Entries</a>
        </div>
        
        <div>
          <a alt="On Learning Elixir" href="on_learning_elixir.html">Next</a>
        </div>
        
      </div>
      <h1>Makeshift Security Camera</h1>
      <p class="article-summary">‚úèÔ∏è Fun project using the Raspberry Pi camera module to catch creeps, thieves, and what-have-you.</p>
      <h4 class="article-date">üóì 07.17.2017</h4>
      <div class="article-body"><h2 id="dangthieves">Dang Thieves</h2>
<p>My wife and I got a little paranoid about thieves coming into our house. Determined to not spend money on a security camera, I set out to build my own working solution. The <a href="https://www.raspberrypi.org/">Raspberry Pi</a> was an obvious choice for this project. I had not done any video-related programming before, so this was a fun challenge. Here's the requirements for my personal surveillance system:</p>
<ul>
<li>Capture video using the Raspberry Pi's camera module</li>
<li>Motion detection!</li>
<li>Stream the live video feed to a web page</li>
<li>Basic controls to manually operate the system remotely from a web browser</li>
<li>Send SMS text messages with a video attachment when motion is detected</li>
</ul>
<p>These are pretty standard features for a security system, however, it was kind of a challenge to get it working as I wanted. As I began coding, I found that some of the features were easy to implement on their own, but trying to combine these features was a real challenge. I hope to share some of my solutions and things that I was able to learn. <a href="https://github.com/denvaar/creeper">Here's a link to all of the source code on GitHub</a>.</p>
<h2 id="motiondetection">Motion Detection</h2>
<p><img src="../assets/arm.png" alt="" /></p>
<p>This is one of those projects that rely heavily on the awesome work that other people have already created. <a href="http://opencv.org/">OpenCV</a> is an open source computer vision library. I used the Python version to capture frames from the pi's camera and to determine if something has moved. There's a lot to how this algorithm works behind the scenes, but here are the basics:</p>
<p>I have a <code>get_frame</code> method that is continuously called. Inside this method is where the processing happens. An image frame is read from the camera device:</p>
<pre><code>_, image = self.video.read() # self.video is instance of cv2.VideoCapture(0)
</code></pre>
<p>With this image from the video device, I resize it and change the colorspace.</p>
<pre><code class="python language-python">frame = imutils.resize(image, width=500)
gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
</code></pre>
<p>At this point the frame looks like this:</p>
<p><img src="../assets/cv-image1.jpg" alt="" /></p>
<p>Then a gaussian blur function is applied to the frame:</p>
<pre><code class="python language-python">gray = cv2.GaussianBlur(gray, (21, 21), 0)
</code></pre>
<p><img src="../assets/cv-image2.jpg" alt="" /></p>
<p>The idea is to compare the contours between two frames. I created an initial frame that is used to compare each new frame against using the <code>absdiff</code> function. It produces a scary ghost-like image that looks like this:</p>
<pre><code class="python language-python">frame_delta = cv2.absdiff(self.initial_frame, gray)
</code></pre>
<p><img src="../assets/cv-image3.jpg" alt="" /></p>
<p>These images are a result of combining the current frame with the initial frame. Finally, using more cv2 image manipulation functions, I end up with an image that looks like this, and represents a difference between the initial frame and the current frame from the camera:</p>
<p><img src="../assets/cv-image4.jpg" alt="" /></p>
<p>I use this image with the <code>cv2.findContours</code> function to get the contours in the image. I picked the value 500 to use as an indicator that enough change has happened to alert the system that motion has been detected.</p>
<pre><code class="python language-python">(_, contours, _) = cv2.findContours(dilated_threshold.copy(),
                                    cv2.RETR_EXTERNAL,
                                    cv2.CHAIN_APPROX_SIMPLE)
for contour in contours:
    contour_area = cv2.contourArea(contour)
    if contour_area &gt;= 500:
        self.last_movement = current_time
        self.movement_detected = True
</code></pre>
<p>I learned a lot about this method of detecting motion from the link at the bottom of the page. Check it out for more information.</p>
<h2 id="streamingvideototheweb">Streaming Video to the Web</h2>
<p><img src="../assets/creeper.png" alt="" /></p>
<p>I was so surprised at how difficult it is to stream video to a web page. I think this was the most challenging part of the project, but once I found a solution, everything started to fall into place.</p>
<p>I decided to go with <a href="http://flask.pocoo.org/">Flask</a> as a web server because (a) I had never used it before, (b) It's very lightweight and easy to set up, and (c) there were good examples online of how to do streaming.</p>
<p>One challenge I ran into towards the end was that I realized my Flask server could only serve one client connection at a time when streaming video. I did some research and learned that you can use <a href="http://www.gevent.org/">gevent</a> to run the server in a way that allows multiple clients to connect asynchronously. To get this to work I had to modify my camera frame generator function to be threaded.</p>
<pre><code class="python language-python"># camera_streamer.py

class CameraStream(object):
    """Stream image frames from a camera source"""
    thread = None
    stream = None
    video_camera = None

    def __init__(self, camera_source, *args, **kwargs):
        if self.thread is None:
            CameraStream.video_camera = camera_source
            CameraStream.thread = Thread(target=self.generate)
            CameraStream.thread.start()

    def request_stream(self):
        """Return current frame from stream"""

        return self.stream

    @classmethod
    def generate(cls):
        """Generate next frame from camera source continuously"""

        while True:
            cls.stream = cls.video_camera.get_frame()
        cls.video_camera = None
        cls.stream = None
        cls.thread = None
</code></pre>
<p>As for the web interface, it's nothing too crazy. An HTML <code>img</code> tag receives frame after frame of encoded JPEG data. In my application this works because Flask allows the usage of a generator function as a response. With the right headers and body you get a nice motion jpeg stream.</p>
<pre><code class="python language-python">def gen(stream_handle):
    while True:
        response = (
            b'--frame\r\n'
            b'Content-Type: image/jpeg\r\n\r\n' +
            stream_handle.request_stream() +
            b'\r\n\r\n'
        )
        yield response

@app.route('/video_feed')
def video_feed():
    return Response(gen(streamer) or None,
                    mimetype='multipart/x-mixed-replace; boundary=frame')
</code></pre>
<p>In the snippet above you can see the generator function that's continuously yielding frames from the camera. The response is formatted with the correct headers and mimetype for the client(s).</p>
<p>Here's the relevant bit of code from <code>index.html</code></p>
<pre><code>&lt;img src="{{ url_for('video_feed') }}"&gt;
</code></pre>
<p>The record and reset buttons on the web interface are just async actions that trigger functions to manipulate the video camera.</p>
<h2 id="smstextnotifications">SMS Text Notifications</h2>
<p><img style="width:400px" src="../assets/sms-example.jpg"></p>
<p>To my delight, there's such a thing as an "SMS gateway," which makes sending text messages from a computer program easy. From what I understand, most carriers have an email address that you can send messages to. <code>1234567890@mms.att.net</code> for example. The carrier routes the message to the mobile phone number provided. If there's an error sending the message, you receive an email instead.</p>
<p>The main issue with sending text messages is the size of the attachment. If it's too big (About 600KB if I am not mistaken) then the message won't be delivered. With that limitation in mind, I set up ffmpeg to limit the video size to about 590KB, which is about 8 seconds. Notifications are sent either once the file size limit has been reached, or as soon as motion is no longer detected.</p>
<p>There's nothing else too intriguing about this part of the code, so I will just <a href="https://github.com/denvaar/creeper/blob/master/notifications.py">link to it here</a>.</p>
<h2 id="takeawaysandfindings">Takeaways and Findings</h2>
<p>I feel like each feature of this project was pretty simple to do. The real challenge was connecting the pieces to work together. Software that bridges the gap between what's on our computer screens and what's in our physical world is extra exiting to create. Receiving a text message after waving your hand in front of a camera is really cool.</p>
<p>One thing I don't like about this project, or that I would change in the future, is that it requires the web server to be on the same machine as the camera. This isn't ideal because my Raspberry Pi isn't the best web server. In the future, it would be cool to perhaps use ffmpeg to stream the frames from the camera to a remote server.</p>
<p>One of the biggest sources of latency is the crappy wifi dongle I have on my pi. It causes the video to lag behind real-time. I don't think there's much I can do about it other than to use ethernet or buy a nicer wifi dongle.</p>
<p>Finally, if I wanted to make this system more robust, I would add some authentication to the web interface. I wouldn't want anyone just monitoring my camera. I just didn't add this feature because it was beyond the scope of what I was trying to learn and accomplish.</p>
<p>As I began to work on this project, I realized I would have to take advantage of concurrency using threads and processes to perform subtasks. By doing so, I was able to improve the performance as well as the overall quality of the security system. This project has sparked an interest in computer vision for me. I don't know if my wife and I will catch any thieves, but they will not be able to take away the things I've learned from creating this makeshift security camera.</p>
<p><img src="https://media.giphy.com/media/d3mlE7uhX8KFgEmY/source.gif" width="300px" /></p>
<p>I found these resources very helpful while working on this project:</p>
<ul>
<li><a href="https://blog.miguelgrinberg.com/post/video-streaming-with-flask/page/3">Miguel Grindberg's article on video streaming with Flask</a></li>
<li><a href="http://www.pyimagesearch.com/2015/05/25/basic-motion-detection-and-tracking-with-python-and-opencv/">Motion detection with OpenCV</a></li>
</ul></div>
    </div>
    <footer class="foot center">¬© 2021 Denver Smith</footer>
    <script src="../js/highlight.pack.js"></script>
    <script>
      hljs.initHighlightingOnLoad();
    </script>
  </body>
</html>
